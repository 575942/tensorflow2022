{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  #词向量化\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  #序列矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、词向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词典\n",
      " {'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
     ]
    }
   ],
   "source": [
    "sentence=[\n",
    "    \"I love my dog\",\n",
    "    \"I love my cat\"\n",
    "]\n",
    "tokenizer=Tokenizer(num_words=100)  #建立100哥单词的词典\n",
    "tokenizer.fit_on_texts(sentence)  #向量化，分局单词出现的频率进行排序\n",
    "word_index=tokenizer.word_index   #查看词向量(dict)\n",
    "print(\"词典\\n\",word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词典\n",
      " {'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "sentence=[\n",
    "    \"I love my dog\",\n",
    "    \"I love my cat\",\n",
    "    \"you love my dog!\"\n",
    "]\n",
    "tokenizer=Tokenizer(num_words=100)  #建立100哥单词的词典\n",
    "tokenizer.fit_on_texts(sentence)  #向量化，分局单词出现的频率进行排序\n",
    "word_index=tokenizer.word_index   #查看词向量(dict)\n",
    "print(\"词典\\n\",word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、词序列化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词典\n",
      " {'love': 1, 'i': 2, 'my': 3, 'dog': 4, 'cat': 5, 'you': 6, 'really': 7, 'and': 8, 'your': 9}\n",
      "['I love my dog', 'I love my cat', 'you really love my dog and i love your cat']\n",
      "序列化\n",
      " [[2, 1, 3, 4], [2, 1, 3, 5], [6, 7, 1, 3, 4, 8, 2, 1, 9, 5]]\n"
     ]
    }
   ],
   "source": [
    "sentence=[\n",
    "    \"I love my dog\",\n",
    "    \"I love my cat\",\n",
    "    \"you really love my dog and i love your cat\"\n",
    "]\n",
    "tokenizer=Tokenizer(num_words=100)  #建立100哥单词的词典\n",
    "tokenizer.fit_on_texts(sentence)  #向量化，分局单词出现的频率进行排序\n",
    "word_index=tokenizer.word_index   #查看词向量(dict)\n",
    "print(\"词典\\n\",word_index)\n",
    "#根据序列\n",
    "seq=tokenizer.texts_to_sequences(sentence)\n",
    "print(sentence)\n",
    "print(\"序列化\\n\",seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词典\n",
      " {'oov': 1, 'love': 2, 'i': 3, 'my': 4, 'dog': 5, 'cat': 6, 'you': 7, 'really': 8, 'and': 9, 'your': 10}\n",
      "序列化\n",
      " [[3, 2, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "'''当词典中出现新的单词时，用oov代替'''\n",
    "sentence=[\n",
    "    \"I love my dog\",\n",
    "    \"I love my cat\",\n",
    "    \"you really love my dog and i love your cat\"\n",
    "]\n",
    "tokenizer=Tokenizer(num_words=100, \n",
    "                    oov_token=\"oov\"  #当词典没有这个单词时，用oov代替\n",
    "                    )  #建立100哥单词的词典\n",
    "tokenizer.fit_on_texts(sentence)  #构建词典\n",
    "word_index=tokenizer.word_index\n",
    "print(\"词典\\n\",word_index)\n",
    "\n",
    "test_data=[\n",
    "    \"I love todoy's weather\"\n",
    "]\n",
    "test_seq=tokenizer.texts_to_sequences(test_data)\n",
    "print(\"序列化\\n\",test_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1构建词矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oov': 1, 'love': 2, 'i': 3, 'my': 4, 'dog': 5, 'cat': 6, 'you': 7, 'really': 8, 'and': 9, 'your': 10}\n",
      "序列化： [[3, 2, 4, 5], [3, 2, 4, 6], [7, 8, 2, 4, 5, 9, 3, 2, 10, 6]]\n",
      "词矩阵\n",
      " [[ 0  0  0  0  0  0  3  2  4  5]\n",
      " [ 0  0  0  0  0  0  3  2  4  6]\n",
      " [ 7  8  2  4  5  9  3  2 10  6]]\n",
      "0设置在后面\n",
      " [[ 3  2  4  5  0  0  0  0  0  0]\n",
      " [ 3  2  4  6  0  0  0  0  0  0]\n",
      " [ 7  8  2  4  5  9  3  2 10  6]]\n",
      "选择从后面丢失信息\n",
      " [[3 2 4 5 0]\n",
      " [3 2 4 6 0]\n",
      " [7 8 2 4 5]]\n"
     ]
    }
   ],
   "source": [
    "sentence=[\n",
    "    \"I love my dog\",\n",
    "    \"I love my cat\",\n",
    "    \"you really love my dog and i love your cat\"\n",
    "]\n",
    "tokenizer=Tokenizer(num_words=100, \n",
    "                    oov_token=\"oov\"  #当词典没有这个单词时，用oov代替\n",
    "                    )  #建立100哥单词的词典\n",
    "tokenizer.fit_on_texts(sentence)  #构建词典\n",
    "#向量化\n",
    "word_index=tokenizer.word_index \n",
    "print(word_index) \n",
    "#序列化\n",
    "seq=tokenizer.texts_to_sequences(sentence) \n",
    "print(\"序列化：\",seq)\n",
    "#构建词序列矩阵\n",
    "pad=pad_sequences(seq)\n",
    "print(\"词矩阵\\n\",pad)\n",
    "\n",
    "pad=pad_sequences(seq,\n",
    "                padding=\"post\"  #将0填充在后面\n",
    "                ) \n",
    "print(\"0设置在后面\\n\",pad)\n",
    "\n",
    "pad=pad_sequences(seq,padding=\"post\",\n",
    "                maxlen=5,  #设置最多列数\n",
    "                truncating=\"post\"  #选择从后面丢失信息\n",
    "                )\n",
    "print(\"选择从后面丢失信息\\n\",pad)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
